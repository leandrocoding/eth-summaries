% Basic stuff
\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[nswissgerman]{babel}
\usepackage{scrextend}
\usepackage{lipsum}
\usepackage{gauss}
\usepackage{amssymb} % to import \leadsto

% 3 column landscape layout with fewer margins
\usepackage[landscape, left=0.75cm, top=1cm, right=0.75cm, bottom=1.5cm, footskip=15pt]{geometry}
\usepackage{flowfram}
\usepackage{floatrow}
\usepackage{amsmath}
\usepackage[inline]{enumitem}
\usepackage{pst-node}
\usepackage{auto-pst-pdf}
\usepackage{tikz}
\usepackage{tikz-cd} 
\usepackage{multicol}

\usetikzlibrary{calc,matrix}

\changefontsizes[9pt]{8pt}
\ffvadjustfalse
\setlength{\columnsep}{1cm}
\Ncolumn{3}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Sign}{sign}
\DeclareMathOperator{\Rank}{Rank}
\DeclareMathOperator{\Image}{Im}
\DeclareMathOperator{\Columnspace}{\mathcal{R}}
\DeclareMathOperator{\Nullspace}{\mathcal{N}}
\DeclareMathOperator{\Kernel}{Ker}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Span}{span}

\newcommand*{\hermconj}{\mathsf{H}}

% define nice looking boxes
\usepackage[most]{tcolorbox}

% a base set, that is then customised
\tcbset {
  base/.style={
    boxrule=0mm,
    leftrule=1mm,
    left=1.75mm,
    arc=0mm, 
    fonttitle=\bfseries, 
    colbacktitle=black!10!white, 
    coltitle=black, 
    toptitle=0.75mm, 
    bottomtitle=0.25mm,
    title={#1}
  }
}

\newcommand{\middot}{~\textperiodcentered~}
\newlist{rowlist}{enumerate*}{1}
\setlist[rowlist]{label={\textbf{\roman*}\text{: }}, afterlabel={}, itemjoin=\middot}

\newlist{vaxioms}{enumerate*}{1}
\setlist[vaxioms]{label={\textbf{V\arabic*}\text{: }}, afterlabel={}, itemjoin=\middot}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newcommand\undermat[2]{%
  \makebox[0pt][l]{$\smash{\underbrace{\phantom{%
    \begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\newcommand\overmat[2]{%
  \makebox[0pt][l]{$\smash{\overbrace{\phantom{%
    \begin{matrix}#2\end{matrix}}}^{\text{$#1$}}}$}#2}

\definecolor{brandblue}{rgb}{0.34, 0.7, 1}
\newtcolorbox{mainbox}[1]{
  colframe=brandblue, 
  base={#1}
}

\newtcolorbox{subbox}[1]{
  colframe=black!20!white,
  base={#1}
}

% Mathematical typesetting & symbols
\usepackage{amsthm, mathtools, amssymb} 
\usepackage{marvosym, wasysym}
\allowdisplaybreaks

% Tables
\usepackage{tabularx, multirow}
\usepackage{booktabs}

% Make enumerations more compact
\usepackage{enumitem}
\setitemize{itemsep=0.5pt}
\setenumerate{itemsep=0.75pt}

% To include sketches & PDFs
\usepackage{graphicx}

% For hyperlinks
\usepackage{hyperref}
\hypersetup{
  colorlinks=true
}

% Metadata
\title{Cheatsheet Lineare Algebra}
\author{Leandro Zazzi}
\date{Januar 2023}

% Math helper stuff
\def\limn{\lim_{n\to \infty}}
\def\limxo{\lim_{x\to 0}}
\def\limxi{\lim_{x\to\infty}}
\def\limxn{\lim_{x\to-\infty}}
\def\sumk{\sum_{k=1}^\infty}
\def\sumn{\sum_{n=0}^\infty}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\E{\mathbb{E}}
\def\K{\mathbb{K}}
\def\dx{\text{ d}x}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\begin{document}


\section{Vorwissen}
\subsection{Komplexe Zahlen}
\begin{mainbox}{Definition}
Ein Ausdruck der Form $z = a + ib$, wobei $i^2 = -1$. $a = Re(z)$ ist der Realteil, $b = Im(z)$ ist der Imaginärteil.
\end{mainbox}

Addition erfolgt komponentenweise, Multiplikation erfolgt unter Annahme des Binomialgesetzes und $i^2 = -1$ (i.e. $z w = (a c - b d) + i (a d + b  c)$). Für Division gilt $\frac{z}{w} = \frac{c + id}{a + ib} = \frac{z\overbar{w}}{w\overbar{w}} = \frac{(ca + bd) + i(ad - cb)}{a^2 + b^2}$.\\
Die Norm ist definiert als $|z| = \sqrt{Re(z)^2 + Im(z)^2} = \sqrt{z \cdot \overbar{z}}$. Für $z = x + iy$ ist $\overbar{z} = x - iy$ konjugiert-komplex.

\begin{subbox}{}
$$z \overbar{z} = Re(z)^2 + Im(z)^2$$
\end{subbox}

Eine komplexe Zahl kann in Polarkoordinaten dargestellt werden. Es gilt $z = re^{i\phi} = r(\cos(\phi) + i\sin(\phi))$.
Radizieren: $\sqrt[n]{a} = z \iff a = z^n \iff |a| e^{i\alpha} = r^n e^{i\phi n}$ wobei $r = \sqrt[n]{|a|}$ und $\phi = \frac{\alpha + 2k\pi}{n}$.

\begin{mainbox}{Fundamentalsatz der Algebra}
  Sei $p(z) = a_n z^n + \cdots + a_0$ ein Polynom mit $a_n \neq 0$ und reellen oder komplexen Koeffizienten $a_i \in \mathbb{C}$. Dann hat $p(z)$ genau $n$ Nullstellen (mit ihren Vielfachen gezählt).
\end{mainbox}

Es gilt $\overbar{z \pm w} = \overbar{z} \pm \overbar{w}$, $\overbar{zw} = \overbar{z}\overbar{w}$, $\overbar{(\frac{z}{w})} = \frac{\overbar{z}}{\overbar{w}}$, $|\overbar{z}| = |z|$, $|z + w| \leq |z| + |w|$, $|zw| = |z| |w|$.

$$
\theta = \begin{cases}
  \arctan(\frac{y}{x}) \textbf{ if z on positive x-axis}\\
  \frac{\pi}{2} \textbf{ if }x=0, y > 0\\
  \pi + \arctan(\frac{y}{x}) \textbf{ if z on negative x-axis}\\
  \frac{3\pi}{2} \textbf{ if }x=0, y < 0
\end{cases}
$$

\subsection{Polynome}

Bei Polynomen mit reellen Nullstellen treten die Nullstellen als komplex-konjugiertes Paar auf. Für Grad 2, verwende $z = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$ um ein Polynom $p(z) = 0$ zu lösen. Für $a z^n + c = 0$, verwende $z = \sqrt[n]{-\frac{c}{a}}$.

Bei einem Polynom über $\mathbb{C}$ mit ungeradem Grad gibt es mindestens eine reelle Nullstelle.

\section{LGS / Gauss}

Ein homogenes LGS hat die Form $Ax = 0$.

\begin{enumerate}
  \item Wir können Zeilen vertauschen, eine Zeile mit $a \in \mathbb{R}\setminus \{0\}$ multiplizieren und Zeilen voneinander subtrahieren bzw. addieren.
  \item Wir schreiben das lineare Gleichungssystem (LGS) in Matrixform.
  \item Wir transformieren das LGS in die Zeilenstufenform.
  \item Wir lösen das LGS von unten nach oben.
\end{enumerate}

\begin{center}
  \includegraphics[width=0.8 \linewidth]{gauss.png}
\end{center}

\textbf{Verträglichkeitsbedingungen (VTB):} Falls die Verträglichkeitsbedingungen $c_{r+1} = \cdots = c_m = 0$ nicht erfüllt sind, so gibt es keine Lösung. Bei homogenen LGS sind die Verträglichkeitsbedingungen immer erfüllt. Nur wenn $r < n$ gibt es also nicht-triviale Lösungen für das homogene LGS.\\

\begin{subbox}{Lösungen}
  $Ax = b$ hat mindestens eine Lösung gdw. ($r = m$) oder ($r < m$ und VTB sind erfüllt). In diesem Fall gibt es 1 Lösung falls $r = n$, andernfalls eine $n - r$ Schar an Lösungen.
  \begin{itemize}
    \item $r = m$: $\begin{cases}
      r = m = n\text{: eindeutige Lösung, regulär}\\
      r < n\text{: }\infty\text{ Lösungen mit } n - r \text{ freien Variablen}
    \end{cases}$\\
    \item $r < m$: $\begin{cases}
      r = n\text{: eindeutige Lösung}\\
      r < n\text{: }\infty\text{ Lösungen mit } n - r \text{ freien Variablen}
    \end{cases}$
  \end{itemize}
\end{subbox}

Es gilt immer $r \leq \min(m, n)$.

\section{Matrizen \& Vektoren}

Eine $m \times n$ Matrix hat $m$ Zeilen und $n$ Spalten wobei das $i,j$ Element mit $a_{i,j}$ oder $(A)_{i,j}$ bezeichnet wird. Ein $m \times 1$ Vektor ist ein \textbf{Spaltenvektor} und ein $1 \times n$ Vektor ist ein \textbf{Zeilenvektor} (n-Tupel). Die Elemente $a_{jj}, j = 1,2,\cdots,\min(m,n)$ heissen \textbf{Diagonalelemente}. Für eine \textbf{Diagonalmatrix} $A$ gilt $(A)_{ij} = 0$ für $i \neq j$. Wir bezeichnen die Matrix dann durch die Elemente auf der Diagonale: $A = \diag(d_{11}, \cdots, d_{nn})$. Für eine \textbf{obere Dreiecksmatrix} gilt: $(R)_{ij} = 0$ für $i > j$. Für eine \textbf{untere Dreiecksmatrix} gilt: $(L)_{ij} = 0$ für $i < j$. 

Wir definieren als $\Tr(A)$ die Summe der Diagonalelemente.

\begin{subbox}{Multiplikation}
  Für eine $m \times n$ Matrix $A$ und eine $n \times p$ Matrix $B$ gilt $(AB)_{ij} = \sum_{k=1}^n a_{ik} b_{kj}$ wobei $AB$ eine $m \times p$ Matrix ist. Generell \textbf{nicht kommutativ}.
\end{subbox}

Falls zwei Matrizen $A, B \in \E^{n \times n}$ kommutieren ($AB = BA$), dann gilt $(AB)^k = A^k B^k$.

\begin{subbox}{Rechenregeln für Matrizen}
  \begin{rowlist}
    \item $(\alpha \beta)A = \alpha(\beta A)$
    \item $(\alpha A)B = \alpha(AB) = A(\alpha B)$
    \item $(\alpha + \beta)A = \alpha A + \beta A$
    \item $\alpha(A + B) = \alpha A + \alpha B$
    \item $A + B = B + A$
    \item $A + (B + C) = (A + B) + C$
    \item $(AB)C = A(BC)$
    \item $(A + B)C = AC + BC$
    \item $A(B + C) = AB + AC$
  \end{rowlist}
\end{subbox}

Eine \textbf{Linearkombination} der Vektoren $a_1, \cdots, a_n$ ist $\alpha_1 a_1 + \cdots + \alpha_n a_n$.\\
Falls $AB = 0$, so sind $A$ und $B$ \textbf{Nullteiler}.
\begin{subbox}{Transponierte Matrizen}
  \textbf{Transponiert}: $A^\top$ wird definiert als $(A^\top)_{ij} = (A)_{ji}$.\\
  \textbf{Hermitesch-transponiert}: $A^\hermconj = (\overbar{A})^\top = \overbar{A^\top}$.
\end{subbox}
Eine Matrix ist \textbf{symmetrisch} falls $A^\top = A$ und \textbf{hermitesch} falls $A^\hermconj = A$ (Diagonale somit reell). \textbf{Schiefsymmetrisch} ist sie falls $A^\top = -A$. 

Sind $A$ und $B$ hermitesche Matrizen, so ist $AB$ genau dann hermitesch falls $AB = BA$. $A^\hermconj A$ und $A A^\hermconj$ sind für alle Matrizen $A$ hermitesch.

\begin{subbox}{Rechenregeln für hermitesche Matrizen}
  \begin{rowlist}
    \item $(A^\top)^\top = A$
    \item $(A^\hermconj)^\hermconj = A$
    \item $(\alpha A)^\top = \alpha A^\top$
    \item $(\alpha A)^\hermconj = \overbar{\alpha} A^\hermconj$
    \item $(A + B)^\top = A^\top + B^\top$
    \item $(A + B)^\hermconj = A^\hermconj + B^\hermconj$
    \item $(AB)^\top = B^\top A^\top$
    \item $(AB)^\hermconj = B^\hermconj A^\hermconj$
  \end{rowlist}
\end{subbox}

\begin{subbox}{Spalten- und Reihensichtweise}
  Ist $A = \begin{psmallmatrix}
    \underline{a_1} \\
    \cdots \\
    \underline{a_m}
  \end{psmallmatrix}$ eine $m \times n$ Matrix, $B = \begin{psmallmatrix}
    b_1 & b_2 & \cdots & b_p
  \end{psmallmatrix}$ eine $n \times p$ Matrix, dann gilt $AB = \begin{psmallmatrix}
    A b_1 & A b_2 & \cdots & A b_p
  \end{psmallmatrix} = \begin{psmallmatrix}
    \underline{a_1} B \\
    \cdots \\
    \underline{a_m} B
  \end{psmallmatrix}$.

  Ist umgekehrt $A = \begin{psmallmatrix}
    a_1 & a_2 & \cdots & a_n
  \end{psmallmatrix}$ eine $m \times n$ Matrix, $B = \begin{psmallmatrix}
    \underline{b_1} \\
    \cdots \\
    \underline{b_n}
  \end{psmallmatrix}$ eine $n \times p$ Matrix, dann gilt $AB = \sum_{i=1}^n a_i \underline{b_i}$.
\end{subbox}

Multipliziert man eine Diagonalmatrix von links, dann skaliert dies die Zeilen. Multipliziert man eine Diagonalmatrix von rechts, dann multipliziert dies die Spalten.

\begin{subbox}{Definite}
  Eine Matrix $A$ ist positiv-definit (positiv-semidefinit), falls $\forall x \in \E^n$, $x^\hermconj A x > 0$ ($\geq 0$).
\end{subbox}

Eine symmetrische Matrix $A$ ist genau dann \textbf{positiv-definit (positiv-semidefinit)} falls alle Eigenwerte strikt positiv (positiv) sind.

Um zu zeigen, dass eine Matrix $A$ indefinit ist, zeigt man, dass es ein $x$ mit $x^\top A x > 0$ und ein $y$ mit $y^\top A y < 0$ gibt. Es gilt $e_i^\top A e_j = a_{ij}$. Folglich ist $A$ indefinit, falls es auf der Diagonalen von $A$ verschiedene Vorzeichen gibt (vorrausgesetzt $\det(A) \neq 0$).

\textbf{Marlene Trick}: Sei $M$ eine symmetrische Matrix und $A_k$ die $k \times k$ Blockmatrix der oberen, linken Ecke. Mit $\Delta_k$ bezeichnen wir die Determinante von $A_k$. Nun ist $M$ pos. def. wenn $\Delta_k$ > 0 für alle $k$, weiter ist $M$ neg. def.
wenn $(-1)^k \Delta_k > 0$ für alle $k$. $A$ ist indefinit falls das erste $\Delta_k$ das keiner der beiden Regeln entspricht ein falsches Vorzeichen hat.\\

Das $\textbf{äussere Produkt}$ für zwei Vektoren $x$ und $y$ ist definiert als $xy^\top$. Die \textbf{orthogonale Projektion} von $x$ auf $y$ ist $P_y x = \frac{1}{\lVert y \rVert^2} y y^\hermconj x$.

Entsprechend ist die \textbf{Orthogonalprojektion} $P_A \coloneqq A(A^\hermconj A)^{-1} A^\hermconj$ (mit $A \in \E^{m \times n}$ und $\Rank A = n$) die Projektion auf den Kolonnenraum $\Columnspace(A)$. Für diese Orthogonalprojektion gilt $\lVert y - Py \rVert_2 = \min_{z \in \Image P} \lVert y - z \rVert_2$ (siehe \textbf{Least Squares}).

Eine Matrix $P$ heisst \textbf{Projektor} falls $P^2 = P$.

\subsection{Inverse}

Eine $n \times n$ Matrix ist $\textbf{invertierbar}$ wenn eine Matrix $A^{-1}$ existiert, so dass $A A^{-1} = I_n = A^{-1} A$. Die Inverse ist eindeutig bestimmt und existiert genau dann wenn $\Rank A = n$.

\begin{subbox}{Rechenregeln für Inverse}
  Sind $A, B$ regulär. Dann:

  \begin{itemize}
    \item $A^{-1}$ regulär und $(A^{-1})^{-1} = A$.
    \item $AB$ regulär und $(AB)^{-1} = B^{-1} A^{-1}$.
    \item $A^\hermconj$ regulär und $(A^\hermconj)^{-1} = (A^{-1})^\hermconj$.
  \end{itemize}
\end{subbox}

Die Inverse findet man durch Gauss-Jordan Elimination.

$$\begin{bmatrix*}
  A | I_n
\end{bmatrix*} \xRightarrow[\text{operationen}]{\text{Zeilen-}} \begin{bmatrix*}
  I_n | A^{-1}
\end{bmatrix*}$$

Für $A = \begin{psmallmatrix*}
  a & b \\
  c & d
\end{psmallmatrix*}$ und $\det(A) \neq 0$ (also $A$ invertierbar) gilt $A^{-1} = \frac{1}{ad - bc} \begin{psmallmatrix*}
  d & -b \\
  -c & a
\end{psmallmatrix*}$.

\subsection{Orthogonale und unitäre Matrizen}

Eine $n \times n$ Matrix heisst \textbf{unitär} falls $A^\hermconj A = I_n$. Eine Matrix heisst \textbf{orthogonal} falls $A^\top A = I_n$. Die Matrix ist genau dann unitär/orthogonal wenn alle Spalten orthonormal sind.

Für $A, B$ unitär (dasselbe für orthogonal) gilt:

\begin{rowlist}
  \item $A$ regulär und $A^{-1} = A^\hermconj$
  \item $AA^\hermconj = I_n = A^\hermconj A$
  \item $A^{-1}$, $AB$ unitär
\end{rowlist}

Für unitäre Matrizen gilt $| \det(A) | = 1$, für orthogonale Matrizen somit $\det(A) = \pm 1$. Ausserdem gilt $| \lambda | = 1$ für alle Eigenwerte $\lambda$ einer unitären Matrix. Entsprechend gilt $\lambda = \pm 1$ für alle \textbf{Eigenwerte einer orthogonalen Matrix}. 

Eine \textbf{Rotationsmatrix} $R$ ist eine unitäre Matrix. In 2D ist $R = \begin{psmallmatrix*}
  \cos \theta & -\sin \theta \\
  \sin \theta & \cos \theta
\end{psmallmatrix*}$. Verallgemeinert wird dies durch Givens-Rotationen. In 4D gilt bspw. $\begin{psmallmatrix}
  \cos \theta & 0 & -\sin \theta & 0 \\
  0 & 1 & 0 & 0 \\
  \sin \theta & 0 & \cos \theta & 0 \\
  0 & 0 & 0 & 1
\end{psmallmatrix}$.

Die \textbf{Householder-Matrix} $Q_u = I - 2uu^\top$ stellt eine Spiegelung an der Hyperebene welche orthogonal zu $u$ ist dar.

\section{LR-Zerlegung}

Die \textbf{LR-Zerlegung} ist ein weiteres Verfahren zum lösen von LGS. Wir erhalten dadurch $PA = LR$. Es ist besonders effektiv wenn wir mehrere LGS mit gleichem $A$ haben.

\begin{align*}
  & \begin{bmatrix}[ccc|ccc|ccc]
    \overmat{I}{1 & 0 & 0} & \overmat{A}{2 & 1 & 2} & \overmat{I}{1 & 0 & 0}\\
    0 & 1 & 0 & \textcolor{green}{1} & 2 & 3 & 0 & 1 & 0\\
    0 & 0 & 1 & \textcolor{blue}{2} & 2 & 2 & 0 & 0 & 1
  \end{bmatrix} \begin{matrix}
    \\
    \textcolor{green}{-\frac{1}{2}}\\
    \textcolor{blue}{-1}
  \end{matrix}\\
  \implies & \begin{bmatrix}[ccc|ccc|ccc]
    1 & 0 & 0 & 2 & 1 & 2 & 1 & 0 & 0\\
    0 & 1 & 0 & 0 & \frac{3}{2} & 2 & \textcolor{green}{\frac{1}{2}} & 1 & 0\\
    0 & 0 & 1 & 0 & \textcolor{cyan}{1} & 0 & \textcolor{blue}{1} & 0 & 1
  \end{bmatrix} \begin{matrix}
    \\
    \\
    \textcolor{cyan}{-\frac{2}{3}}
  \end{matrix}\\
  \implies & \begin{bmatrix}[ccc|ccc|ccc]
    1 & 0 & 0 & 2 & 1 & 2 & 1 & 0 & 0\\
    0 & 1 & 0 & 0 & \frac{3}{2} & 2 & \textcolor{green}{\frac{1}{2}} & 1 & 0\\
    \undermat{P}{0 & 0 & 1} & \undermat{R}{0 & 0 & -\frac{4}{3}} & \undermat{L}{\textcolor{blue}{1} & \textcolor{cyan}{\frac{2}{3}} & 1}
  \end{bmatrix}
\end{align*}\\

Wenn wir Zeilen in $R$ vertauschen, dann vertauschen wir auch die selben Zeilen in $P$ und $L$ (die $1$ Einträge auf der Diagonalen in $L$ werden nicht vertauscht!). Im Allgemeinen gilt $A \in \E^{m \times n}$ sowie $P, L \in \E^{m \times m}$ und $R \in \E^{m \times n}$. Die Matrix $L$ hat immer $1$ auf der Diagonalen und Einträge unten links sind nur ungleich $0$ für Spalten kleiner oder gleich $\Rank A$. Die $i$-te Spalte in $L$ entspricht also dem $i$-ten Pivot von $A$.

Haben wir eine LR-Zerlegung von $A$ können wir $Ax = b$ effizienter lösen. Zuerst lösen wir dazu $PA = LR$ und lösen $Lc = Pb$ nach $c$. Dann lösen wir $Rx = c$ nach $x$.

\section{Vektorräume}

\begin{mainbox}{}
  Eine Vektorraum $V$ über $\K$ ist eine nichtleere Menge auf der eine Vektoraddition und Skalarmultiplikation definiert sind.
  \begin{vaxioms}
    \item $x + y = y + x$
    \item $(x + y) + z = x + (y + z)$
    \item $\exists 0 \in V: x + 0 = x$ ($\forall x \in V$)
    \item $\forall x \in V$ existiert $-x$: $x + (-x) = 0$
    \item $\alpha(x + y) = \alpha x + \alpha y$
    \item $(\alpha + \beta)x = \alpha x + \beta x$
    \item $(\alpha \beta)x = \alpha (\beta x)$
    \item $\exists 1 \in \K$ so dass $\forall x \in V: 1x = x$
  \end{vaxioms}
\end{mainbox}

\begin{rowlist}
  \item $0x = 0$
  \item $\alpha 0 = 0$
  \item $\alpha x = 0 \implies x = 0 \vee \alpha = 0$
  \item $(-\alpha)x = \alpha (-x) = -(\alpha x)$
\end{rowlist}

\subsection{Unterräume}

Ein Unterraum $U$ ist eine nichtleere Teilmenge von $V$ der abgeschlossen bzgl. Addition und Multiplikation ist. $U$ beinhaltet immer den Nullvektor. Jeder Unterraum ist ein Vektorraum. Für zwei Unterräume $U, W \subseteq V$ ist $U \cup W$ genau dann ein Unterraum von $V$ falls $U \subseteq W$ oder $W \subseteq U$. $U \cap W$ ist hingegen immer ein Unterraum.\\

Für eine lineare Abbildung $F: X \mapsto Y$ und einen Unterraum $U \subseteq X$ ist $FU \subseteq Y$ auch ein Unterraum. Ist $W \subseteq \Image F$ ein Unterraum so ist auch $F^{-1}W \subseteq X$ ein Unterraum.

\begin{subbox}{Lineare Hülle ($\Span$)}
  Die Menge der Linearkombinationen der Vektoren $v_1, \cdots, v_n$ ist der Unterraum aufgespannt durch diese Vektoren. Man bezeichnet ihn mit $\Span \{ v_1, \cdots, v_n \}$.
\end{subbox}

Falls $\Span \{v_1, \cdots, v_n\} = V$, dann sind $v_1, \cdots, v_n$ ein \textbf{Erzeugendensystem} von $V$.

Um zu zeigen dass $U \subseteq V$ ein Unterraum von $V$ ist zeigt man 
\begin{rowlist}
  \item $0_V \in U$
  \item $\forall x, y \in U$ gilt $x + y \in U$
  \item $\forall x \in U, \forall \alpha \in \K$ gilt $\alpha x \in U$
\end{rowlist}.

Unterräume sind bspw. auch $\Kernel(A)$ und $\Image(A)$.

\subsection{Lineare Abhängigkeit, Basen und Dimensionen}

\begin{mainbox}{Lineare Unabhängigkeit}
  Vektoren $v_1, \cdots, v_n$ sind linear unabhängig genau dann wenn:
  $$\sum_{k=1}^n \alpha_k v_k = 0 \implies \alpha_1 = \cdots = \alpha_n = 0$$
  Andernfalls sind die Vektoren linear abhängig, d.h. es existiert eine nicht-triviale Nullsumme bzw. ein Vektor lässt sich als Linearkombination der anderen schreiben.
\end{mainbox}

\begin{mainbox}{Basis}
  Ein Erzeugendensystem $v_1, \cdots, v_n$ von $V$ ist eine Basis von $V$ genau dann wenn $v_1, \cdots, v_n$ linear unabhängig sind. Jeder Vektor in $V$ lässt sich eindeutig als Linearkombination der Basisvektoren schreiben.
\end{mainbox}

Jede Menge $\{v_1, \cdots, v_m\} \subseteq V$ mit $\dim V < m$ ist linear abhängig. In jedem endlichen Vektorraum $V$ mit $\dim V = n$ ist eine Menge von $n$ linear unabhängigen Vektoren eine Basis.

In einem Vektorraum $V$ beeinflusst die Wahl der Skalare die Dimension. $\C^n$ über $\C$ hat Dimension $n$, $\C^n$ über $\R$ hat Dimension $2n$.

Die Koeffizienten $\xi = (\xi_1, \cdots, \xi_n)^\top$ sind \textbf{Koordinaten} von $x$ bzgl. einer Basis $\mathcal{B}$ falls $x = \sum_{i=1}^n \xi_i b_i$ gilt. Die Summe wird Koordinatendarstellung genannt.

Zwei Unterräume $U, W \subseteq V$ heissen \textbf{komplementär} falls jedes $x \in V$ eine \textbf{eindeutige} Darstellung $x = u + w$ mit $u \in U$ und $w \in W$ hat ($V$ ist dann direkte Summe von $U$ und $W$). Man schreibt $V = U \oplus W$. Falls zwei Unterräume komplementär sind, folgt $U \cap W = \{0_V\}$. Um zu \textbf{zeigen dass $U \oplus W = V$ gilt} zeigt man $\dim V = \dim U + \dim W$ und $U \cap W = \{0_V\}$.

Falls  $f: U \rightarrow V$ und $g: V \rightarrow W$ lineare Abbildungen zwischen Vektorräumen sind so dass $ g \circ f$ ein Isomorphismus ist, dann gilt $V = \operatorname{Im}f \oplus \operatorname{Ker} g$.

\subsection{Basiswechsel und Koordinatentransformation}

Wenn wir von einer alten Basis $\mathcal{B}$ zu einer neuen Basis $\mathcal{B}'$ wechseln, können wir die neue Basis mit der alten darstellen. Es gilt dann $b_k' = \sum_{i=1}^n \tau_{ik} b_i$ mit der \textbf{Basiswechselmatrix} $T = (\tau_{ik})$.

Es gilt $\xi = T \xi'$ sowie $\xi' = T^{-1} \xi$, da $T$ regulär ist. Dabei sind $\xi'$ die Koordinaten in der Basis $\mathcal{B}'$ und $\xi$ sind die Koordinaten in der Basis $\mathcal{B}$. Es gilt dann $\mathcal{B}' = \mathcal{B} T$, was aus $B \xi = x = B' \xi'$ und $\xi = T \xi'$ folgt. Wir schreiben auch $T = \text{Mat}(\mathcal{B'})_{\mathcal{B}}$.

Wir können $T$ zwischen $\mathcal{B}$ und $\mathcal{B}'$ auch durch Gauss-Elimination finden (denn $T = \mathcal{B}^{-1} \mathcal{B'}$):

$$\begin{bmatrix*}
  \mathcal{B} | \mathcal{B}'
\end{bmatrix*} \xRightarrow[\text{operationen}]{\text{Zeilen-}} \begin{bmatrix*}
  I_n | T
\end{bmatrix*}$$

\section{Lineare Abbildungen}

Eine Abbildung $F: V \mapsto W$ ist linear, wenn $F(v + w) = F(v) + F(w)$ sowie $F(\alpha v) = \alpha F(v)$ für alle $v, w \in V$ und $\alpha \in \K$ gilt. Einfacher ausgedrückt ist $F$ linear falls $F(\alpha v + w) = \alpha F(v) + F(w)$. 

Für eine Funktion $F: X \mapsto Y$ gilt:
\begin{itemize}
  \item \textbf{injektiv:} $\forall x, x' \in X$ $f(x) = f(x') \implies x = x'$
  \item \textbf{surjektiv:} $\forall y \in Y$ gibt es $x \in X$ mit $f(x) = y$
  \item \textbf{bijektiv:} injektiv und surjektiv, $F^{-1}$ existiert
\end{itemize}

Sei $F: X \mapsto Y$ eine lineare Abbildung ist wobei $\Span \{ b_1, \cdots, b_n \} = X$ und $\Span \{ c_1, \cdots, c_m \} = Y$ Basen sind.

Dann lässt sich $F(b_l) = \sum_{k=1}^m a_{kl} c_k$ schreiben. Die Matrix $A^{m \times n}$ mit Elementen $a_{kl}$ heisst die Abbildungsmatrix bezüglich $X, Y$. Jede lineare Abbildung lässt sich also durch eine Matrix darstellen.

Ist $F$ bijektiv ist es ein Isomorphismus, ist zusätzlich $X = Y$ so ist $F$ ein Automorphismus. Wenn $F$ ein Isomorphismus ist, existiert ein Isomorphismus $F^{-1}$.

\subsection{Kern, Bild und Rang}


Sei $F: X \mapsto Y$ eine lineare Abbildung. Es gilt $\Rank F \coloneqq \dim \Image F$.

\begin{mainbox}{Kern und Bild}

  Wir definieren den \textbf{Kern} als $\Kernel F \coloneqq \{x \in X | F(x) = 0\}$. Der Kern ist ein Unterraum von $X$ und $F$ ist injektiv genau dann wenn $\Kernel F = \{0\}$

  Wir definieren das \textbf{Bild} als $\Image F \coloneqq \{F(x) | x \in X\}$. Das Bild ist ein Unterraum von $Y$ und $F$ ist surjektiv genau dann wenn $\Image F = Y$.
\end{mainbox}

\begin{mainbox}{Rangsatz}
  $$\dim X - \dim \Kernel F = \dim \Image F = \Rank F$$
\end{mainbox}

Zwei Vektorräume endlicher Dimension sind genau dann isomorph wenn sie die gleiche Dimension haben.

\begin{subbox}{Foldergungen des Rangsatzes}
  Seien $F: X \mapsto Y, G: Y \mapsto Z$ lineare Abbildungen:
  \begin{itemize}
    \item $F$ injektiv $\iff$ $\Rank F = \dim X$
    \item $F$ surjektiv $\iff$ $\Rank F = \dim Y$
    \item $F$ bijektiv $\iff$ $\Rank F = \dim X = \dim Y$
    \item $\Rank G \circ F \leq \min(\Rank F, \Rank G)$
    \item $G$ injektiv $\implies$ $\Rank GF = \Rank F$
    \item $F$ surjektiv $\implies$ $\Rank GF = \Rank G$
  \end{itemize}
\end{subbox}

\subsection{Matrizen als lineare Abbildungen}

Sei $A \in \E^{m \times n}$. Der \textbf{Kolonnenraum} oder Wertebereich von $A$ ist der Unterraum $\Columnspace(A) = \Image A = \Span \{ a_1, \cdots, a_n \} \subseteq \E^m$. Der \textbf{Nullraum} von $A$ ist der Unterraum $\Nullspace(A) = \Kernel A \subseteq \E^n$.


\begin{subbox}{Rangsatz für Matrizen}
  Sei $r \coloneqq \Rank A = \dim \Image A$. Dann ist $\dim \Kernel A = n - r$ und $r$ entspricht der Anzahl Pivotelemente in REF. Zusätzlich:
  
  $$\Rank A^\top = \Rank A^\hermconj = \Rank A$$
\end{subbox}

Somit gilt für Matrizen $A \in \E^{m \times n}$ und $B \in \E^{p \times m}$ auch:

\begin{itemize}
  \item $\Rank BA \leq \min(\Rank B, \Rank A)$
  \item $\Rank B = m \leq p \implies \Rank BA = \Rank A$
  \item $\Rank A = m \leq n \implies \Rank BA = \Rank B$
\end{itemize}

Für quadratische Matrizen $A \in \E^{n \times n}$ sind also folgende Aussagen äquivalent:

\begin{rowlist}
  \item $A$ ist regulär
  \item $\Rank A = n$
  \item Spalten (oder Zeilen) sind linear unabhängig
  \item $\Kernel A = \{0\}$
  \item $\Image A = \E^n$
\end{rowlist}

Falls $x_0$ eine Lösung für $Ax = b$ ist, so ist die Lösungsmenge $\mathcal{L}_b = x_0 + \Kernel A$ ein affiner Unterraum.

\subsection{Zusammenfassende Eigenschaften von $A$}

Sei $A \in M^{m \times n}$ mit $r \coloneqq \Rank A$:

\begin{rowlist}
  \item $\dim \Image A = \dim \Image A^\hermconj = r$
  \item $\dim \Kernel A = n - r$
  \item $\dim \Kernel A^\hermconj = m - r$
\end{rowlist}

\begin{align*}
  r = n & \Leftrightarrow \Kernel A = \{ 0 \} & r = m & \Leftrightarrow \Kernel A^\hermconj = \{ 0 \} \\
  & \Leftrightarrow \text{Spalten von $A$ l.u.} & & \Leftrightarrow \text{Spalten von $A$ erzeugend}\\
  & \Leftrightarrow \text{Zeilen von $A$ erzeugend} & & \Leftrightarrow \text{Zeilen von $A$ l.u.}\\
  & \Leftrightarrow A \text{ injektiv} & & \Leftrightarrow A \text{ surjektiv}
\end{align*}

\begin{subbox}{Basis für $\Image A$ und $\Kernel A$ finden}
  Zuerst bringt man $A$ auf Zeilenstufenform.
  \begin{itemize}
    \item \textbf{Basis für $\Image A$}: Die Spalten in der \textit{originalen} Matrix welche den Pivotspalten in der Zeilenstufenform entsprechen sind Basisvektoren für $\Image A$.
    \item \textbf{Basis für $\Kernel A$}: Zuerst parameterisiert man die freien Variablen und findet dann die Lösungesmenge $\mathcal{L}_0$ von $Ax = 0$. Die Zeile $i$ des Lösungsvektors enthält dann den Wert der $i$-ten Variable. Man wählt dann die Koeffizienten der freien Variablen entsprechend um Basisvektoren zu finden (e.g. man setzt einen der Koeffizienten auf $1$ und alle anderen auf $0$ pro Basisvektor). 
  \end{itemize}
\end{subbox}

\subsection{Abbildungen von Koordinatentransformation}

\begin{subbox}{}
  \begin{multicols}{2}
    \begin{tikzcd}
      x \in X \arrow{r}{F} \arrow[shift left=1]{d}{\kappa_X}  & y \in Y \arrow[shift left=1]{d}{\kappa_Y} \\
      \xi \in \E^n \arrow{r}{A} \arrow[shift left=1]{u}{\kappa_X^{-1}} \arrow[shift left=1]{d}{T^{-1}} & \eta \in \E^m \arrow[shift left=1]{u}{\kappa_Y^{-1}} \arrow[shift left=1]{d}{S^{-1}} \\
      \xi' \in \E^n \arrow[shift left=1]{u}{T} \arrow{r}{B} & \eta' \in \E^m \arrow[shift left=1]{u}{S}
    \end{tikzcd}
    \begin{itemize}
      \item $A = SBT^{-1}$
      \item $B = S^{-1}AT$
      \item $\Rank F = \Rank A = \Rank B$
    \end{itemize}
  \end{multicols}
\end{subbox}

Ist $F$ $\Rank F = r$, so besitzt $F$ bzgl. geeigneten Basen $X, Y$ die Abbildungsmatrix: 
\[
  \left[\begin{array}{ c | c }
    I_r & 0 \\
    \hline
    0 & 0
  \end{array}\right]
\]

\section{Vektorräume mit Skalarprodukt}

\begin{subbox}{Skalarprodukt}
  Skalarprodukt in einem Vektorraum $V$ über $\mathbb{E}$ ist eine Funktion $\langle \cdot, \cdot \rangle : V \times V \mapsto \mathbb{E}$ mit folgenden Eigenschaften:
  \begin{itemize}
    \item Linear im zweiten Faktor: $\langle x, y + z \rangle = \langle x, y \rangle + \langle x, z \rangle$ und $\langle x, \alpha y \rangle = \alpha \langle x, y \rangle$. Für reelle Skalare auch linear im ersten Faktor.
    \item  Symmetrisch wenn $\mathbb{E} = \mathbb{R}$ und hermitesch wenn $\mathbb{E} = \mathbb{C}$: $\langle x, y \rangle = \overbar{\langle y, x \rangle}$
    \item Positiv definit: $\forall x \in V$, $\langle x, x \rangle \in \mathbb{R}$ (auch für komplexe Vektorräume!), $\langle x, x \rangle \geq 0$, $\langle x, x \rangle = 0 \iff x = 0$
  \end{itemize}
  Falls $\mathbb{E} = \mathbb{C}$, nennt man $V$ auch einen unitären Vektorraum, für $\mathbb{E} = \mathbb{R}$ auch euklidischer oder orthogonaler Vektorraum. Für ein Skalarprodukt definiert man die induzierte Norm als $\lVert x \rVert = \sqrt[]{\langle x, x \rangle}$.
\end{subbox}

\begin{subbox}{Norm}
  Norm in Vektorraum $V$ über $\mathbb{E}$ ist eine Funktion $\lVert \cdot \rVert: V \mapsto \mathbb{R}$ mit:
  \begin{itemize}
    \item positiv definit: $\forall x \in V$, $\lVert x \rVert \geq 0$ und $\lVert x \rVert = 0 \iff x = 0$
    \item homogen: $\lVert \alpha x \rVert = \lvert \alpha \rvert \cdot \lVert x \rVert$ $\forall x \in V, \forall \alpha \in \mathbb{E}$
    \item Dreiecksungleichung: $\lVert x + y \rVert \leq \lVert x \rVert + \lVert y \rVert$ $\forall x, y \in V$
  \end{itemize}
  Ein Vektorraum mit einer Norm heisst normierter Vektorraum.
\end{subbox}

\begin{subbox}{Euklidischer Raum}
  Das euklidische Skalarprodukt ist definiert als: $$\langle x, y \rangle \coloneqq x^\hermconj y$$
  Die euklidische 2-Norm ist entsprechend definiert als: $$\lVert x \rVert_2 = \sqrt[]{x^\hermconj x}$$
\end{subbox}

Die $p$-Norm ist definiert als $\lVert x \rVert_p = (|x_1|^p + \cdots + |x_n|^p)^{\frac{1}{p}}$.

\begin{subbox}{Skalarprodukte}
  $\langle x, y \rangle_V$ ist ein Skalarprodukt genau dann wenn $\langle x, y \rangle_V \coloneqq x^\hermconj A y$ wobei $A$ eine hermitesche Matrix mit strikt positiven Eigenwerten ist (positiv-definit). Es gilt dann $a_{ij} = \langle e_i, e_j \rangle$.
\end{subbox}

\begin{mainbox}{Cauchy-Schwarz}
  Sei $V$ ein Vektorraum über $\mathbb{E}$ mit beliebigem Skalarprodukt.
  $$| \langle x, y \rangle | \leq \lVert x \rVert \lVert y \rVert \iff | \langle x, y \rangle |^2 \leq \langle x, x \rangle \langle y, y \rangle$$
  Das Gleichheitszeichen gilt genau dann wenn $x$ und $y$ linear abhängig sind.
\end{mainbox}

\begin{subbox}{Winkel}
  Seien $x, y, \in V$. Der Winkel zwischen $x$ und $y$ ist gegeben als:
  $$\varphi \coloneqq \arccos \frac{Re \langle x, y \rangle}{\lVert x \rVert \lVert y \rVert}$$
\end{subbox}

Zwei Vektoren sind \textbf{orthogonal}, falls $\langle x, y \rangle = 0$ ($x \perp y$). Zwei Teilmengen sind orthogonal, wenn $\forall x \in M, \forall y \in N$ gilt: $\langle x, y \rangle = 0$ ($M \perp N$).

Eine Basis ist \textbf{orthogonal} wenn $\langle b_k, b_l \rangle = \delta_{kl}$ für alle $k, l$, wobei das Kroneckerdelta definiert ist als
$\delta_{kl} = \begin{cases}
  0\text{ falls }k \neq l \\
  1\text{ falls }k = l
\end{cases}$.

Gilt zusätzlich $\lVert b_i \rVert = 1$ $\forall i$ ist sie \textbf{orthonormal}.

\begin{subbox}{Pythagoras}
  Falls $x \perp y$, so folgt $\lVert x \pm y \rVert^2 = \lVert x \rVert^2 + \lVert y \rVert^2$. Allgemein gilt $\lVert x - y \rVert^2 = \lVert x \rVert^2 + \lVert y \rVert^2 - 2 \lVert x \rVert \lVert y \rVert \cos (\varphi)$.
\end{subbox}

Eine Menge von paarweise orthogonalen Vektoren ist linear unabhängig wenn alle Vektoren ungleich null sind. Der Nullvektor ist orthogonal zu allen Vektoren.

Für eine Orthonormalbasis $\{b_1, \cdots, b_n\}$ und $x \in V$ gilt $x = \sum_{k=1}^n \langle b_k, x \rangle b_k$.

\begin{mainbox}{Parsevalsche Formel}
  Seien $x, y \in V$, $\{ b_1, \cdots, b_n \}$ eine Orthonormalbasis, so dass $\xi_k \coloneqq \langle b_k, x \rangle_V$ und $\eta_k \coloneqq \langle b_k, y \rangle_V$. Für $k = 1, \cdots, n$ sind $\xi_k$ und $\eta_k$ die Koordinatenvektoren. 

  $$\langle x, y \rangle_V = \Sigma_{k=1}^n \overbar{\xi_k} \eta_k = \xi^\hermconj \eta = \langle \xi, \eta \rangle_{\E^n}$$

  \begin{rowlist}
    \item $\lVert x \rVert_V = \lVert \xi \rVert_{\E^n}$
    \item $\angle (x,y)_V = \angle (\xi, \eta)_{\E^n}$
    \item $x \perp y \iff \xi \perp \eta$
  \end{rowlist}
\end{mainbox}

\subsection{Gram-Schmidt-Orthonormalisierungsverfahren}

\begin{subbox}{}
  Für einen Unterraum $U$ von $V$ ist $U^\bot \coloneqq \{ y \in V | \{y\} \bot U \}$ das \textbf{orthogonale Komplement}. $U$ und $U^\bot$ sind komplementäre Unterräume in direkter Summe zu $V$.
\end{subbox}

\begin{mainbox}{Algorithmus}
  Sei $\{a_1, \cdots, a_n\}$ eine Menge linear unabhängiger Vektoren. Wir definieren rekursiv:
  \begin{itemize}
    \item $b_1 \coloneqq \frac{a_1}{\lVert a_1 \rVert}$
    \item $\widetilde{b_k} \coloneqq a_k - \sum_{j=1}^{k-1} \langle b_j, a_k \rangle b_j$ \hfill $b_k \coloneqq \frac{\widetilde{b_k}}{\lVert \widetilde{b_k} \rVert}$
  \end{itemize}

  Nach $k$ Schritten sind $\{b_1, \cdots b_k\}$ paarweise orthonormal und $\Span \{a_1, \cdots, a_k\} = \Span \{b_1, \cdots, b_k\}$. Wenn $\{a_1, \cdots, a_n\}$ eine Basis von $V$ ist, ist $\{b_1, \cdots b_n\}$ auch eine. Jeder endlichdimensionale Vektorraum hat eine Orthonormalbasis.
\end{mainbox}

\begin{mainbox}{Fundamentale Unterräume einer Matrix}
  Sei $A \in \E^{m \times n}$ mit $r \coloneqq \Rank A$.
  \begin{align*}
    & \Nullspace(A) = \Columnspace(A^\hermconj)^\perp \subseteq \E^n & & \Nullspace(A) \oplus \Columnspace(A^\hermconj) = \E^n\\
    & \Nullspace(A^\hermconj) = \Columnspace(A)^\perp \subseteq \E^m & & \Nullspace(A^\hermconj) \oplus \Columnspace(A) = \E^m\\
    & \dim \Columnspace(A) = r & & \dim \Nullspace(A) = n - r\\
    & \dim \Columnspace(A^\hermconj) = r & & \dim \Nullspace(A^\hermconj) = m - r
  \end{align*}
\end{mainbox}

\subsection{Basiswechsel und Koordinatentransformation von Orthonormalbasen}

Wir wollen von $\mathcal{B}$ nach $\mathcal{B}'$, wobei beides Orthonormalbasen sind. Wir können $b_k' = \sum_{j=1}^n \tau_{jk} b_j$ schreiben und erhalten die Basiswechselmatrix $T$. Da $\mathcal{B}, \mathcal{B}'$ orthonormal sind gilt $T^\hermconj = T^{-1}$.

Daher gilt $\xi = T \xi'$ und $\xi' = T^\hermconj \xi$. Zudem ist $\mathcal{B} = \mathcal{B}' T$ und $\mathcal{B}' = \mathcal{B} T^\hermconj$, wobei alle Matrizen unitär/orthogonal sind. Die Transformationsmatrix einer Basistransformation zwischen Orthonormalbasen ist unitär/orthogonal.

$T$ ist ausserdem \textbf{längen- und winkeltreu}. 

\begin{mainbox}{Definition}
  Eine lineare Abbildung $F: X \mapsto Y$ ist unitär/orthogonal falls $\langle F(v), F(w) \rangle_Y = \langle v, w \rangle_X$.
\end{mainbox}

Es gilt dann dass \begin{rowlist}
  \item $F$ ist längen- und winkeltreu
  \item $\Kernel F = \{0\}$, injektiv
\end{rowlist}.

Haben wir zusätzlich $\dim X = \dim Y$ gilt \begin{rowlist}
  \item $F$ ist ein Isomorphismus
  \item $\{b_1, \cdots, b_n\}$ ist eine Orthonormalbasis von $X$ $\iff$ $\{F(b_1), \cdots, F(b_n)\}$ ist eine Orthonormalbasis von $Y$
  \item $F^{-1}$ unitär/orthogonal
  \item Abbildungsmatrix $A$ ist unitär/orthogonal
\end{rowlist}.

\section{QR-Zerlegung}

Eine Matrix kann dargestellt werden als $A = QR$ wobei $Q$ orthogonal und $R$ eine obere Dreiecksmatrix ist.

\begin{itemize}
  \item Wende Gram-Schmidt auf den Spalten von $A$ an ($\implies Q$)
  \item $R = Q^\top A$ lösen und $R$ zu erhalten (alternativ $r_{11} = \lVert a_1 \rVert, r_{jk} = \langle q_j, a_k \rangle, r_{kk} = \lVert \widetilde{q_k} \rVert$)
\end{itemize}

Im allgemeinen Fall ist dabei für $A \in \E^{m \times n}$ $Q \in \E^{m \times m}$ und $R \in \E^{m \times n}$. Man definiert dann:

$$A = QR = \begin{bmatrix}
  \widetilde{Q} | \widetilde{Q}_\perp\\
\end{bmatrix} \begin{bmatrix}
  \widetilde{R}\\
  \hline
  \mathbf{0}
\end{bmatrix} = \widetilde{Q} \widetilde{R}$$

wobei $\widetilde{Q} \in \E^{m \times r}, \widetilde{R} \in \E^{r \times n}$.

\section{Least Squares}

Sei $A \in E^{m \times n}$ $Ax = b$ ein überbestimmtes LGS ($m > n$). Im Allgemeinen gibt es keine Lösung, wir möchten deshalb $\lVert Ax - b \rVert_2^2$ minimieren. Wir definieren also $x^\ast = \text{argmin}_{x \in \E^n} \lVert Ax - b \rVert_2^2$. Dies trifft für alle $x \in E^n$ für welche $Ax - b$ \textbf{senkrecht} auf $\Columnspace(A)$ steht. 

\begin{align*}
  Ax^\ast - b \perp \Columnspace(A) & \iff Ax^\ast - b \in \Nullspace(A^\hermconj)\\
  & \iff A^\hermconj(Ax^\ast - b) = 0
\end{align*}

Es gilt $\Nullspace(A^\hermconj A) = \Nullspace(A)$, denn $A^\hermconj A x = 0 \implies x^\hermconj A^\hermconj Ax \implies \lVert Ax \rVert_2^2 = 0 \iff Ax = 0$ und $Ax = 0 \implies A^\hermconj Ax = 0$.

\begin{subbox}{Normalengleichungen}
  Eine Lösung des Least Squares Problems findet man durch lösen der Normalengleichungen:
  $$A^\hermconj A x = A^\hermconj b$$
  Ist $\Rank A = \Rank A^\hermconj A = n \leq m$, so ist $A^\hermconj A$ invertierbar und $x^\ast = (A^\hermconj A)^{-1} A^\hermconj b$.
\end{subbox}

Man nennt $A^+ = (A^\hermconj A)^{-1} A^\hermconj$ auch die Pseudoinverse von $A$, da $A^+ A = I$. Haben wir eine QR-Zerlegung von $A$, gilt $R x^\ast = Q^T b$ für $A = QR$.\\

Für $A \in \E^{m \times n}$ mit $\Rank A = n \leq m$ ist die Least Squares Lösung eindeutig bestimmt. Wenden wir Gram-Schmidt auf die Spalten von $A$ $a_1, \cdots, a_n$ und den Vektor $a_{n+1} \coloneqq y$ an ergibt sich die eindeutige Lösung $Ax^\ast = y - \widetilde{b}_{n+1}$ durch das Lot $\widetilde{b}_{n+1} = y - Ax^\ast \perp \Columnspace(A)$.


\section{Determinanten}

\begin{mainbox}{Determinante}
  Die Determinante einer quadratischen Matrix $A \in \E^{n \times n}$ ist definiert als:
  $$\det(A) = \sum_{p \in S_n} \Sign(p) a_{1,p(1)} \cdots a_{n,p(n)}$$
  $S_n$ ist dabei die symmetrische Gruppe $n$ mit Ordnung $n!$. Jede Permutation $p$ kann als Produkt von Transpositionen (Permutation bei welcher nur zwei Elemente vertauscht werden) dargestellt werden.
  $$\Sign(p) = \begin{cases}
    1 & \text{$p$ Produkt gerader Anzahl Transpositionen} \\
    -1 & \text{$p$ Produkt ungerader Anzahl Transpositionen}
  \end{cases}$$
\end{mainbox}

Für $3 \times 3$ Matrizen gilt (Sarrus):
% https://tex.stackexchange.com/a/32981/184539
$$\begin{tikzpicture}[>=stealth]
  \matrix [%
    matrix of math nodes,
    column sep=1em,
    row sep=1em
  ] (sarrus) {%
    a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
    a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
    a_{31} & a_{32} & a_{33} & a_{31} & a_{32} \\
  };

  \path ($(sarrus-1-1.north west)-(0.5em,0)$) edge ($(sarrus-3-1.south west)-(0.5em,0)$)
        ($(sarrus-1-3.north east)+(0.5em,0)$) edge ($(sarrus-3-3.south east)+(0.5em,0)$)
        (sarrus-1-1)                          edge            (sarrus-2-2)
        (sarrus-2-2)                          edge[->]        (sarrus-3-3)
        (sarrus-1-2)                          edge            (sarrus-2-3)
        (sarrus-2-3)                          edge[->]        (sarrus-3-4)
        (sarrus-1-3)                          edge            (sarrus-2-4)
        (sarrus-2-4)                          edge[->]        (sarrus-3-5)
        (sarrus-3-1)                          edge[dashed]    (sarrus-2-2)
        (sarrus-2-2)                          edge[->,dashed] (sarrus-1-3)
        (sarrus-3-2)                          edge[dashed]    (sarrus-2-3)
        (sarrus-2-3)                          edge[->,dashed] (sarrus-1-4)
        (sarrus-3-3)                          edge[dashed]    (sarrus-2-4)
        (sarrus-2-4)                          edge[->,dashed] (sarrus-1-5);

  \foreach \c in {1,2,3} {\node[anchor=south] at (sarrus-1-\c.north) {$+$};};
  \foreach \c in {1,2,3} {\node[anchor=north] at (sarrus-3-\c.south) {$-$};};
\end{tikzpicture}$$

Für $1 \times 1$ Matrizen gilt $\det(a_{11}) = a_{11}$ und für $2 \times 2$ Matrizen gilt $\det \begin{psmallmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
\end{psmallmatrix} = a_{11} a_{22} - a_{12} a_{21}$.

Für \textbf{Blockdreiecksmatrizen} gilt:

$$
  \det \left[\begin{array}{ c | c }
    A & C \\
    \hline
    0 & B
  \end{array}\right] = \det(A) \det(B)$$


\begin{subbox}{Eigenschaften der Determinante}
  Für die Determinante einer Matrix $A \in \E^{n \times n}$ gilt:
  \begin{enumerate}
    \item{
      $\det$ ist eine \textbf{lineare Funktion} in jeder \textbf{Zeile}.\\
      $$\det \begin{psmallmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \gamma a_{l1} + \gamma' a_{l1}' & \gamma a_{l2} + \gamma' a_{l2}' & \cdots & \gamma a_{ln} + \gamma' a_{ln}' \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
      \end{psmallmatrix}$$
      $$ = \gamma \begin{psmallmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{l1} & a_{l2} & \cdots & a_{ln} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
      \end{psmallmatrix} + \gamma' \begin{psmallmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{l1}' & a_{l2}' & \cdots & a_{ln}' \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
      \end{psmallmatrix}$$
    }
    \item Vertauscht man zwei Zeilen, dann wechselt $\det(A)$ das Vorzeichen
    \item $\det(I) = 1$
  \end{enumerate}
  Die Determinante ist die einzige auf $\E^{n \times n}$ definierte Funktion mit den obigen drei Eigenschaften.
\end{subbox}

\begin{subbox}{Folgerungen für $\det(A), A \in \E^{n \times n}$}
  \begin{itemize}
    \item {
      $\det(A) = 0 \iff A$ ist singulär
      \begin{itemize}
        \item hat $A$ also eine Nullzeile oder Nullspalte, so ist $\det(A) = 0$
        \item hat $A$ zwei gleiche Spalten oder zwei gleiche Zeilen, so ist $\det(A) = 0$
      \end{itemize}
    }
    \item Addiert man zu einer Zeile (oder Spalte) eine andere Zeile (oder Spalte) multipliziert mit einem Skalar, dann ändert sich $\det(A)$ nicht
    \item $\det(\gamma A) = \gamma^n \det(A)$
    \item ist $A$ eine Diagonalmatrix oder Dreiecksmatrix, so ist $\det(A) = \prod_{i=1}^n a_{ii}$
    \item wenden wir Gauss auf $A$ an so gilt $\det(A) = (-1)^v \prod_{k=1}^n r_{kk}$ wobei $v$ die Anzahl Zeilenvertauschungen ist und $r_{kk}$ die Pivotwerte der REF sind
    \item{
      $\det(A^\hermconj) = \overbar{\det(A)}$ sowie $\det(A^\top) = \det(A)$ (auch für komplexe Matrizen)
      \begin{itemize}
        \item wenn man zwei Spalten vertauscht, dann wechselt $\det(A)$ das Vorzeichen
        \item $\det$ ist eine lineare Funktion in jeder Spalte
      \end{itemize}
    }
    \item $\det(AB) = \det(A) \det(B)$ und $\det(A^{-1}) = \frac{1}{\det(A)}$ falls $A^{-1}$ existiert
  \end{itemize}
\end{subbox}

\begin{subbox}{Kofaktor }
  Zu jedem Element $a_{kl}$ einer $n \times n$ Matrix $A$ ist $A_{[k,l]}$ definiert als die $(n-1) \times (n-1)$ Matrix mit der Zeile $k$ und Spalte $l$ gestrichen. Für den Kofaktor $\kappa_{kl}$ von $a_{kl}$ gilt dann:
  $$\kappa_{kl} = (-1)^{k+l} \det(A_{[k,l]})$$
  Dann gilt für jedes feste $k, l$ (Entwicklung nach Zeile $k$ bzw. Spalte $l$):
  \begin{align*}
    & \det(A) = \sum_{i=1}^n a_{ki} \kappa_{ki} & \det(A) = \sum_{i=1}^n a_{il} \kappa_{il} &
  \end{align*}
\end{subbox}

\section{Eigenwerte und Eigenvektoren}

\begin{mainbox}{Definition}
  Eine Zahl $\lambda \in \K$ heisst Eigenwert der linearen Abbildung $F: V \mapsto V$ falls es $v \in V$ mit $v \neq 0$ gibt so dass $F(v) = \lambda v$. $v$ ist dabei ein Eigenvektor. 
\end{mainbox}

Eine lineare Abbildung $F$ und ihre Matrixdarstellung haben die gleichen Eigenwerte und die Eigenvektoren sind über die Koordinatenabbildung $\kappa_V$ verbunden.

\begin{subbox}{Spektrum}
  Die Menge aller Eigenwerte von $F$ heisst \textbf{Spektrum} von $F$ und wird mit $\sigma(F)$ bezeichnet.
\end{subbox}

Die Menge aller Eigenvektoren die zu einem Eigenwert $\lambda$ gehören bilden einen Unterraum $E_\lambda = \{ v \in V | F(v) = \lambda v \}$.

$\lambda$ ist ein Eigenwert von $A$ genau dann wenn $A - \lambda I$ einen nicht-trivialen Nullraum hat. Es gilt dann $E_\lambda = \Kernel(A - \lambda I)$. Die \textbf{geometrische Vielfachheit} von $\lambda$ entspricht dann $\dim E_\lambda$.

\begin{subbox}{Charakteristisches Polynom}
  Das charakteristische Polynom von $A$ ist das Polynom $\chi_A(\lambda) = \det(A - \lambda I)$. $\chi_A(\lambda) = 0$ ist dabei die charakteristische Gleichung.
  $$\chi_A(\lambda) = (-1)^n \lambda^n + (-1)^{n-1} \Tr(A) \lambda^{n-1} + \cdots + \det(A)$$
  Die \textbf{algebraische Vielfachheit} eines Eigenwertes $\lambda$ ist die Vielfachheit von $\lambda$ als Nullstelle von $\chi_A$ über $\C$.
\end{subbox}

Sei $A \in \E^{2 \times 2}$. Dann ist das charakteristische Polynom $\chi_A(\lambda) = \lambda^2 - \Tr(A) \lambda + \det(A)$.

Für \textbf{schiefsymmetrische Matrizen} sind alle Eigenwerte entweder $0$ oder imaginär. Bei \textbf{Dreiecksmatrizen} entsprechen die Eigenwerte den Werten auf der Diagonalen.

\begin{subbox}{Eigenschaften von Eigenwerten und Eigenvektoren}
  \begin{itemize}
    \item{
      Für jede Matrix $A \in \mathbb{E}^{n \times n}$ gilt:
      \begin{align*}
        & \det(A) = \prod_{i=1}^{n} \lambda_i & & \Tr(A) = \Sigma_{i=1}^n \lambda_i
      \end{align*}
    }
    \item $\lambda$ ist ein Eigenwert von $A$ genau dann wenn $\lambda$ eine Nullstelle von $\chi_A$ ist
    \item Für einen Eigenwert $\lambda$ gilt:{
      $$1 \leq \text{geo. Vfh. von } \lambda \leq \text{alg. Vfh. von } \lambda \leq n$$
    }
    \item Eigenvektoren zu verschiedenen Eigenwerten sind linear unabhängig
    \item Eine Matrix ist genau dann singulär wenn sie $0$ als Eigenwert hat
  \end{itemize}
\end{subbox}

\begin{subbox}{Eigenwerte und Eigenvektoren finden}
  \begin{enumerate}
    \item Bestimmte das charakteristische Polynom $\chi_A(\lambda) = \det(A - \lambda I)$
    \item Bestimme die Nullstellen $\lambda_1, \lambda_2, \cdots, \lambda_n$ von $\chi_A$ (Eigenwerte)
    \item Für jedes $\lambda_k$ bestimme eine Basis für $\Kernel(A - \lambda_k I)$ (Eigenvektoren)
  \end{enumerate}
\end{subbox}

\begin{subbox}{Ähnliche Matrizen}
  Zwei Matrizen $A, C \in \E^{n \times n}$ sind ähnlich falls es ein $T \in \E^{n \times n}$ gibt so dass $C = T^{-1} A T$.

  Ähnliche Matrizen haben das gleiche charakteristische Polynom, die gleiche Spur, die gleiche Determinante, den selben Rang und die gleichen Eigenwerte inkl. algebraischer und geometrischer Vielfachheit (im Allgemeinen aber nicht die selben Eigenvektoren).
\end{subbox}

\section{Spektral- und Eigenwertzerlegung}

Eine Matrix $A \in \E^{n \times n}$ besitzt genau dann eine Eigenbasis wenn es eine ähnliche Diagonalmatrix $\Lambda$ gibt. Man nennt die Matrix dann diagonalisierbar.

$$A \underbrace{\begin{bsmallmatrix}
  \vert & \vert & \vert \\
  v_1   & \cdots & v_n   \\
  \vert & \vert & \vert
\end{bsmallmatrix}}_V = \underbrace{\begin{bsmallmatrix}
  \vert & \vert & \vert \\
  v_1   & \cdots & v_n   \\
  \vert & \vert & \vert
\end{bsmallmatrix}}_V \underbrace{\begin{bsmallmatrix}
  \lambda_1 & \cdots & 0 \\
  \vdots & \ddots & \vdots \\
  0 & \cdots & \lambda_n
\end{bsmallmatrix}}_\Lambda$$ $$ A = V \Lambda V^{-1}, Av_j = v_j \lambda_j, j = 1, \cdots, n$$

$A$ wird durch $V$ diagonalisiert. Die Kolonnen von $V$ sind Eigenvektoren, die Diagonalelemente von $\Lambda$ Eigenwerte. Eine Matrix ist genau dann diagonalisierbar wenn für alle Eigenwerte \textbf{die geometrische Vielfachheit der algebraischen Vielfachheit entspricht}. Dies trifft immer zu wenn alle $n$ Eigenwerte verschieden sind.

\begin{subbox}{Spektralsatz}
  Sei $A \in \E^{n \times n}$ hermitesch ($A^\hermconj = A$). Dann gilt:
  \begin{itemize}
    \item Alle Eigenwerte sind reell
    \item Die Eigenvektoren sind paarweise orthogonal
    \item Es gibt eine orthonormale Basis aus Eigenvektoren $U$ so dass $A = U \Lambda U^\hermconj$
  \end{itemize}
  Dasselbe gilt für reell-symmetrische Matrizen. In diesem Fall sind die Eigenvektoren reell. Für Matrizen mit reellen Eigenwerten und einer reellen Orthonormalbasis aus Eigenvektoren gilt auch die Umkehrung.
\end{subbox}

\begin{subbox}{Normale Matrizen}
  Eine Matrix $A \in \E^{n \times n}$ ist normal, falls $A^\hermconj A = A A^\hermconj$. $A$ ist genau dann diagonalisierbar durch eine unitäre Matrix über $\C$ falls $A$ normal ist. 
\end{subbox}

\section{Singulärwertszerlegung}

Die Singulärwertszerlegung existiert für jede Matrix $A \in \E^{m \times n}$. $A^\hermconj A$ ist immer hermitesch und positiv-definit. Es existiert also eine Spektralzerlegung:

\begin{align*}
  A^\hermconj A V = V \Lambda & \xRightarrow[\text{Umordnung}]{\lambda = \sigma^2} & A^\hermconj A V_r = V_r \Sigma_r^2\\
  & \implies & V_r^\hermconj A^\hermconj A V_r = \Sigma_r^2\\
  & \implies & \underbrace{(\Sigma_r^{-1} V_r^\hermconj A^\hermconj)}_{U_r^\hermconj (r \times m)} \underbrace{(A V_r \Sigma_r^{-1})}_{U_r (m \times r)} = I\\
\end{align*}

$U_r$ kann zu einer unitären $m \times m$ Matrix $U$ ergänzt werden. $U, V$ sind unitär, $\Sigma$ ist nicht-negativ, reell und diagonal mit $\sigma_1 \geq \cdots \sigma_r > \sigma_{r+1} = \cdots = \sigma_n = 0$ wobei $r = \Rank A = \Rank A^\hermconj A = \Rank A A^\hermconj$. $V$ diagonalisiert $A^\hermconj A$, $U$ diagonalisiert $A A^\hermconj$.

\begin{mainbox}{Singulärwertszerlegung}
  $$A = U \Sigma V^\hermconj = \sum_{k=1}^r u_k \sigma_k v_k^\hermconj$$
\end{mainbox}

$$AA^\hermconj = U \Sigma_m^2 U^\hermconj, A^\hermconj A = V \Sigma_n^2 V^\hermconj$$

Ausserdem gilt $A^\hermconj = V \Sigma^\top U^\hermconj$ und falls $A$ invertierbar ist $A^{-1} = V \Sigma^{-1} U^\hermconj$. Aus $AV = U \Sigma$ und $A^\hermconj U = V \Sigma^\top$ folgt:

\begin{itemize}
  \item $\{u_1, \cdots, u_r\}$: Orthonormalbasis von $\Image A = \Columnspace(A)$
  \item $\{u_{r+1}, \cdots, u_m\}$: Orthonormalbasis von $\Kernel A^\hermconj = \Nullspace(A^\hermconj)$
  \item $\{v_1, \cdots, v_r\}$: Orthonormalbasis von $\Image A^\hermconj = \Columnspace(A^\hermconj)$
  \item $\{v_{r+1}, \cdots, v_n\}$: Orthonormalbasis von $\Kernel A = \Nullspace(A)$
\end{itemize}

Bei einer \textbf{Selbstabbildung} gilt:
$$A = U \Sigma V^\hermconj = V \underbrace{V^\hermconj U}_R \Sigma V^\hermconj = V R \Sigma V^\hermconj$$
wobei $R$ eine Rotation/Spiegelung ist und $\Sigma$ die Hauptachsen skaliert.

$A V_r = U_r \Sigma_r$ ist die reduzierte Form der Singulärwertszerlegung. Es gilt dabei $U_r^\hermconj U_r = I$ sowie $V_r^\hermconj V_r = I$.

Aus der Singulärwertszerlegung folgt auch dass die $r$ positiven \textbf{Eigenwerte von $A^\hermconj A$ und $AA^\hermconj$} gleich sind, die Vielfachheit des Eigenwertes $0$ aber $n-r$ respektive $m-r$ ist.

\section{Synthetische Division}
Berechne $\frac{6x^3 + 5x^2 - 7}{3x^2 - 2x - 1} = 2x + 3 + \frac{8x - 4}{3x^2 -2x - 1}$:\\
\begin{center}
  \includegraphics[width=0.4 \linewidth]{synthetic-division.png}
\end{center}

\section{Trigonometrie}

\begin{itemize}
  \item $\sin(z) = \frac{e^{iz} - e^{-iz}}{2i}$
  \item $\cos(z) = \frac{e^{iz} + e^{-iz}}{2}$
  \item $\sin(x) - \sin(y) = 2\sin(\frac{x - y}{2})\cos(\frac{x + y}{2})$
  \item $\cos(x) - \cos(y) = -2\sin(\frac{x - y}{2})\sin(\frac{x + y}{2})$
  \item $\sin(\alpha + \beta) = \sin(\alpha) \cos(\beta) + \cos(\alpha) \sin(\beta)$
  \item $\cos(\alpha + \beta) = \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta)$
\end{itemize}

\begin{mainbox}{}
  \begin{center} 
   \begin{tabular}{c|cccccc}
    deg & 0° & 30° & 45° & 60° & 90° & 180° \\
    \midrule
    rad & 0 & $\frac{\pi}{6}$ & $\frac{\pi}{4}$ & $\frac{\pi}{3}$ & $\frac{\pi}{2}$ & $\pi$ \\
    cos & 1 & $\frac{\sqrt{3}}{2}$ & $\frac{\sqrt{2}}{2}$ & $\frac{1}{2}$ & 0 & -1 \\
    sin & 0 & $\frac{1}{2}$ & $\frac{\sqrt{2}}{2}$ & $\frac{\sqrt{3}}{2}$ & 1 & 0 \\
    tan & 0 & $\frac{1}{\sqrt{3}}$ & 1 & $\sqrt{3}$ & $+\infty$ & 0 \\
   \end{tabular}
  \end{center}
\end{mainbox}

\end{document}
